{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torch.optim import lr_scheduler\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "from torch.utils.data import Subset\n",
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organizing the dataset\n",
    "data_dir = '/raid/ee-mariyam/maryam/ayush/PlantVillage'\n",
    "train_dir = data_dir + '/train'\n",
    "valid_dir = data_dir + '/val'\n",
    "nThreads = 4\n",
    "batch_size = 32\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "import json\n",
    "\n",
    "# Define your transforms for the training and validation sets\n",
    "# Data augmentation and normalization for training\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "included_subdirs = ['Tomato___healthy',\n",
    " 'Tomato___Late_blight',\n",
    " 'Tomato___Septoria_leaf_spot',\n",
    " 'Tomato___Target_Spot',\n",
    " 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
    " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
    " 'Tomato___Bacterial_spot',\n",
    " 'Tomato___Early_blight',\n",
    " 'Tomato___Leaf_Mold',\n",
    " 'Tomato___Tomato_mosaic_virus']\n",
    "image_datasets = {}\n",
    "for x in ['train', 'val']:\n",
    "    dir_path = os.path.join(data_dir, x)\n",
    "    included_paths = [os.path.join(dir_path, subdir) for subdir in included_subdirs]\n",
    "    dataset = datasets.ImageFolder(dir_path, transform=data_transforms[x])\n",
    "\n",
    "    # Filter samples and update class indexing\n",
    "    filtered_samples = [\n",
    "        (path, class_idx)\n",
    "        for path, class_idx in dataset.samples\n",
    "        if os.path.dirname(path) in included_paths\n",
    "    ]\n",
    "    if len(filtered_samples) == 0:\n",
    "        raise ValueError(f\"No samples found in the included subdirectories for '{x}' split.\")\n",
    "\n",
    "    filtered_paths, filtered_targets = zip(*filtered_samples)\n",
    "    unique_targets = sorted(set(filtered_targets))\n",
    "    target_mapping = {class_idx: i for i, class_idx in enumerate(unique_targets)}\n",
    "\n",
    "    filtered_targets = [target_mapping[class_idx] for class_idx in filtered_targets]\n",
    "    dataset.samples = list(zip(filtered_paths, filtered_targets))\n",
    "    dataset.targets = filtered_targets\n",
    "    dataset.classes = [dataset.classes[class_idx] for class_idx in unique_targets]\n",
    "\n",
    "    image_datasets[x] = dataset\n",
    "\n",
    "# Using the image datasets and the trainforms, define the dataloaders\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=2)\n",
    "              for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=dataloaders['train']\n",
    "test_loader=dataloaders['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from timm.models import create_model\n",
    "\n",
    "# Define model (choose appropriate ConvNeXt variant)\n",
    "model = create_model('convnext_base', pretrained=True)  # Adjust variant (e.g., 'convnext_small', 'convnext_base')\n",
    "\n",
    "# Freeze pre-trained layers for fine-tuning (optional)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers\n",
    "\n",
    "# Modify the final classifier layer for the number of PlantVillage classes\n",
    "num_classes = 10  # Get number of classes from dataset\n",
    "model.head.fc = nn.Linear(model.head.fc.in_features, num_classes)  # Replace final layer\n",
    "\n",
    "# Unfreeze the last few layers for fine-tuning (optional)\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True  # Unfreeze final layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 334.079MB\n",
      "number of parameters: 344\n"
     ]
    }
   ],
   "source": [
    "# Size and number of parameters of model\n",
    "param_size = 0\n",
    "num_parameters = 0\n",
    "for param in model.parameters():\n",
    "    num_parameters += 1\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "print('number of parameters: '+ str(num_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNeXt(\n",
      "  (stem): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (stages): Sequential(\n",
      "    (0): ConvNeXtStage(\n",
      "      (downsample): Identity()\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "          (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "          (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (3): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (4): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (5): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (6): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (7): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (8): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (9): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (10): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (11): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (12): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (13): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (14): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (15): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (16): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (17): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (18): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (19): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (20): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (21): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (22): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (23): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (24): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (25): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (26): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "          (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): ConvNeXtStage(\n",
      "      (downsample): Sequential(\n",
      "        (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "      )\n",
      "      (blocks): Sequential(\n",
      "        (0): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (1): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "        (2): ConvNeXtBlock(\n",
      "          (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "          (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (act): GELU()\n",
      "            (drop1): Dropout(p=0.0, inplace=False)\n",
      "            (norm): Identity()\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (drop2): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (shortcut): Identity()\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm_pre): Identity()\n",
      "  (head): NormMlpClassifierHead(\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
      "    (norm): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (pre_logits): Identity()\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (fc): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.1726, Train Accuracy: 65.39%\n",
      "Epoch 2/10, Loss: 0.6139, Train Accuracy: 83.32%\n",
      "Epoch 3/10, Loss: 0.4752, Train Accuracy: 87.33%\n",
      "Epoch 4/10, Loss: 0.4005, Train Accuracy: 89.02%\n",
      "Epoch 5/10, Loss: 0.3520, Train Accuracy: 90.38%\n",
      "Epoch 6/10, Loss: 0.3185, Train Accuracy: 91.18%\n",
      "Epoch 7/10, Loss: 0.2937, Train Accuracy: 91.84%\n",
      "Epoch 8/10, Loss: 0.2766, Train Accuracy: 92.58%\n",
      "Epoch 9/10, Loss: 0.2604, Train Accuracy: 92.75%\n",
      "Epoch 10/10, Loss: 0.2406, Train Accuracy: 93.13%\n",
      "Training finished, took 1100.54s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume model and train_loader are defined elsewhere\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Fine-tune the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "training_start_time = time.time()\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    loss_values.append(avg_loss)\n",
    "    train_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "print('Training finished, took {:.2f}s'.format(time.time() - training_start_time))\n",
    "\n",
    "# Plot the training loss curve\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), loss_values, marker='o')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 96.34%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "running_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    # logits = outputs.logits\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    # Calculate training accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total_predictions += labels.size(0)\n",
    "    correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_predictions / total_predictions\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvNeXtStage(\n",
      "    (downsample): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=128)\n",
      "        (norm): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvNeXtStage(\n",
      "    (downsample): Sequential(\n",
      "      (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(128, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
      "        (norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): ConvNeXtStage(\n",
      "    (downsample): Sequential(\n",
      "      (0): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(256, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (3): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (4): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (5): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (6): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (7): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (8): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (9): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (10): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (11): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (12): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (13): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (14): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (15): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (16): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (17): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (18): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (19): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (20): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (21): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (22): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (23): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (24): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (25): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (26): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(512, 512, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=512)\n",
      "        (norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): ConvNeXtStage(\n",
      "    (downsample): Sequential(\n",
      "      (0): LayerNorm2d((512,), eps=1e-06, elementwise_affine=True)\n",
      "      (1): Conv2d(512, 1024, kernel_size=(2, 2), stride=(2, 2))\n",
      "    )\n",
      "    (blocks): Sequential(\n",
      "      (0): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (1): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "      (2): ConvNeXtBlock(\n",
      "        (conv_dw): Conv2d(1024, 1024, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1024)\n",
      "        (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (act): GELU()\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (shortcut): Identity()\n",
      "        (drop_path): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-grad-cam' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/jacobgil/pytorch-grad-cam.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./pytorch-grad-cam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: Pillow in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 4)) (0.15.2)\n",
      "Requirement already satisfied: ttach in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 5)) (0.0.3)\n",
      "Requirement already satisfied: tqdm in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 6)) (4.66.2)\n",
      "Requirement already satisfied: opencv-python in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 7)) (4.9.0.80)\n",
      "Requirement already satisfied: matplotlib in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 8)) (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from -r pytorch-grad-cam/requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: filelock in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (4.10.0)\n",
      "Requirement already satisfied: sympy in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: jinja2 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (69.1.1)\n",
      "Requirement already satisfied: wheel in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (0.42.0)\n",
      "Requirement already satisfied: cmake in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (3.27.2)\n",
      "Requirement already satisfied: lit in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from triton==2.0.0->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (16.0.6)\n",
      "Requirement already satisfied: requests in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from torchvision>=0.8.2->-r pytorch-grad-cam/requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from scikit-learn->-r pytorch-grad-cam/requirements.txt (line 9)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from scikit-learn->-r pytorch-grad-cam/requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from scikit-learn->-r pytorch-grad-cam/requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->-r pytorch-grad-cam/requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from jinja2->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from requests->torchvision>=0.8.2->-r pytorch-grad-cam/requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from requests->torchvision>=0.8.2->-r pytorch-grad-cam/requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from requests->torchvision>=0.8.2->-r pytorch-grad-cam/requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /raid/ee-mariyam/maryam/miniconda3/envs/ayush/lib/python3.8/site-packages (from requests->torchvision>=0.8.2->-r pytorch-grad-cam/requirements.txt (line 4)) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /raid/ee-mariyam/maryam/.local/lib/python3.8/site-packages (from sympy->torch>=1.7.1->-r pytorch-grad-cam/requirements.txt (line 3)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r pytorch-grad-cam/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_h5=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stem\n",
      "stem.0\n",
      "stem.1\n",
      "stages\n",
      "stages.0\n",
      "stages.0.downsample\n",
      "stages.0.blocks\n",
      "stages.0.blocks.0\n",
      "stages.0.blocks.0.conv_dw\n",
      "stages.0.blocks.0.norm\n",
      "stages.0.blocks.0.mlp\n",
      "stages.0.blocks.0.mlp.fc1\n",
      "stages.0.blocks.0.mlp.act\n",
      "stages.0.blocks.0.mlp.drop1\n",
      "stages.0.blocks.0.mlp.norm\n",
      "stages.0.blocks.0.mlp.fc2\n",
      "stages.0.blocks.0.mlp.drop2\n",
      "stages.0.blocks.0.shortcut\n",
      "stages.0.blocks.0.drop_path\n",
      "stages.0.blocks.1\n",
      "stages.0.blocks.1.conv_dw\n",
      "stages.0.blocks.1.norm\n",
      "stages.0.blocks.1.mlp\n",
      "stages.0.blocks.1.mlp.fc1\n",
      "stages.0.blocks.1.mlp.act\n",
      "stages.0.blocks.1.mlp.drop1\n",
      "stages.0.blocks.1.mlp.norm\n",
      "stages.0.blocks.1.mlp.fc2\n",
      "stages.0.blocks.1.mlp.drop2\n",
      "stages.0.blocks.1.shortcut\n",
      "stages.0.blocks.1.drop_path\n",
      "stages.0.blocks.2\n",
      "stages.0.blocks.2.conv_dw\n",
      "stages.0.blocks.2.norm\n",
      "stages.0.blocks.2.mlp\n",
      "stages.0.blocks.2.mlp.fc1\n",
      "stages.0.blocks.2.mlp.act\n",
      "stages.0.blocks.2.mlp.drop1\n",
      "stages.0.blocks.2.mlp.norm\n",
      "stages.0.blocks.2.mlp.fc2\n",
      "stages.0.blocks.2.mlp.drop2\n",
      "stages.0.blocks.2.shortcut\n",
      "stages.0.blocks.2.drop_path\n",
      "stages.1\n",
      "stages.1.downsample\n",
      "stages.1.downsample.0\n",
      "stages.1.downsample.1\n",
      "stages.1.blocks\n",
      "stages.1.blocks.0\n",
      "stages.1.blocks.0.conv_dw\n",
      "stages.1.blocks.0.norm\n",
      "stages.1.blocks.0.mlp\n",
      "stages.1.blocks.0.mlp.fc1\n",
      "stages.1.blocks.0.mlp.act\n",
      "stages.1.blocks.0.mlp.drop1\n",
      "stages.1.blocks.0.mlp.norm\n",
      "stages.1.blocks.0.mlp.fc2\n",
      "stages.1.blocks.0.mlp.drop2\n",
      "stages.1.blocks.0.shortcut\n",
      "stages.1.blocks.0.drop_path\n",
      "stages.1.blocks.1\n",
      "stages.1.blocks.1.conv_dw\n",
      "stages.1.blocks.1.norm\n",
      "stages.1.blocks.1.mlp\n",
      "stages.1.blocks.1.mlp.fc1\n",
      "stages.1.blocks.1.mlp.act\n",
      "stages.1.blocks.1.mlp.drop1\n",
      "stages.1.blocks.1.mlp.norm\n",
      "stages.1.blocks.1.mlp.fc2\n",
      "stages.1.blocks.1.mlp.drop2\n",
      "stages.1.blocks.1.shortcut\n",
      "stages.1.blocks.1.drop_path\n",
      "stages.1.blocks.2\n",
      "stages.1.blocks.2.conv_dw\n",
      "stages.1.blocks.2.norm\n",
      "stages.1.blocks.2.mlp\n",
      "stages.1.blocks.2.mlp.fc1\n",
      "stages.1.blocks.2.mlp.act\n",
      "stages.1.blocks.2.mlp.drop1\n",
      "stages.1.blocks.2.mlp.norm\n",
      "stages.1.blocks.2.mlp.fc2\n",
      "stages.1.blocks.2.mlp.drop2\n",
      "stages.1.blocks.2.shortcut\n",
      "stages.1.blocks.2.drop_path\n",
      "stages.2\n",
      "stages.2.downsample\n",
      "stages.2.downsample.0\n",
      "stages.2.downsample.1\n",
      "stages.2.blocks\n",
      "stages.2.blocks.0\n",
      "stages.2.blocks.0.conv_dw\n",
      "stages.2.blocks.0.norm\n",
      "stages.2.blocks.0.mlp\n",
      "stages.2.blocks.0.mlp.fc1\n",
      "stages.2.blocks.0.mlp.act\n",
      "stages.2.blocks.0.mlp.drop1\n",
      "stages.2.blocks.0.mlp.norm\n",
      "stages.2.blocks.0.mlp.fc2\n",
      "stages.2.blocks.0.mlp.drop2\n",
      "stages.2.blocks.0.shortcut\n",
      "stages.2.blocks.0.drop_path\n",
      "stages.2.blocks.1\n",
      "stages.2.blocks.1.conv_dw\n",
      "stages.2.blocks.1.norm\n",
      "stages.2.blocks.1.mlp\n",
      "stages.2.blocks.1.mlp.fc1\n",
      "stages.2.blocks.1.mlp.act\n",
      "stages.2.blocks.1.mlp.drop1\n",
      "stages.2.blocks.1.mlp.norm\n",
      "stages.2.blocks.1.mlp.fc2\n",
      "stages.2.blocks.1.mlp.drop2\n",
      "stages.2.blocks.1.shortcut\n",
      "stages.2.blocks.1.drop_path\n",
      "stages.2.blocks.2\n",
      "stages.2.blocks.2.conv_dw\n",
      "stages.2.blocks.2.norm\n",
      "stages.2.blocks.2.mlp\n",
      "stages.2.blocks.2.mlp.fc1\n",
      "stages.2.blocks.2.mlp.act\n",
      "stages.2.blocks.2.mlp.drop1\n",
      "stages.2.blocks.2.mlp.norm\n",
      "stages.2.blocks.2.mlp.fc2\n",
      "stages.2.blocks.2.mlp.drop2\n",
      "stages.2.blocks.2.shortcut\n",
      "stages.2.blocks.2.drop_path\n",
      "stages.2.blocks.3\n",
      "stages.2.blocks.3.conv_dw\n",
      "stages.2.blocks.3.norm\n",
      "stages.2.blocks.3.mlp\n",
      "stages.2.blocks.3.mlp.fc1\n",
      "stages.2.blocks.3.mlp.act\n",
      "stages.2.blocks.3.mlp.drop1\n",
      "stages.2.blocks.3.mlp.norm\n",
      "stages.2.blocks.3.mlp.fc2\n",
      "stages.2.blocks.3.mlp.drop2\n",
      "stages.2.blocks.3.shortcut\n",
      "stages.2.blocks.3.drop_path\n",
      "stages.2.blocks.4\n",
      "stages.2.blocks.4.conv_dw\n",
      "stages.2.blocks.4.norm\n",
      "stages.2.blocks.4.mlp\n",
      "stages.2.blocks.4.mlp.fc1\n",
      "stages.2.blocks.4.mlp.act\n",
      "stages.2.blocks.4.mlp.drop1\n",
      "stages.2.blocks.4.mlp.norm\n",
      "stages.2.blocks.4.mlp.fc2\n",
      "stages.2.blocks.4.mlp.drop2\n",
      "stages.2.blocks.4.shortcut\n",
      "stages.2.blocks.4.drop_path\n",
      "stages.2.blocks.5\n",
      "stages.2.blocks.5.conv_dw\n",
      "stages.2.blocks.5.norm\n",
      "stages.2.blocks.5.mlp\n",
      "stages.2.blocks.5.mlp.fc1\n",
      "stages.2.blocks.5.mlp.act\n",
      "stages.2.blocks.5.mlp.drop1\n",
      "stages.2.blocks.5.mlp.norm\n",
      "stages.2.blocks.5.mlp.fc2\n",
      "stages.2.blocks.5.mlp.drop2\n",
      "stages.2.blocks.5.shortcut\n",
      "stages.2.blocks.5.drop_path\n",
      "stages.2.blocks.6\n",
      "stages.2.blocks.6.conv_dw\n",
      "stages.2.blocks.6.norm\n",
      "stages.2.blocks.6.mlp\n",
      "stages.2.blocks.6.mlp.fc1\n",
      "stages.2.blocks.6.mlp.act\n",
      "stages.2.blocks.6.mlp.drop1\n",
      "stages.2.blocks.6.mlp.norm\n",
      "stages.2.blocks.6.mlp.fc2\n",
      "stages.2.blocks.6.mlp.drop2\n",
      "stages.2.blocks.6.shortcut\n",
      "stages.2.blocks.6.drop_path\n",
      "stages.2.blocks.7\n",
      "stages.2.blocks.7.conv_dw\n",
      "stages.2.blocks.7.norm\n",
      "stages.2.blocks.7.mlp\n",
      "stages.2.blocks.7.mlp.fc1\n",
      "stages.2.blocks.7.mlp.act\n",
      "stages.2.blocks.7.mlp.drop1\n",
      "stages.2.blocks.7.mlp.norm\n",
      "stages.2.blocks.7.mlp.fc2\n",
      "stages.2.blocks.7.mlp.drop2\n",
      "stages.2.blocks.7.shortcut\n",
      "stages.2.blocks.7.drop_path\n",
      "stages.2.blocks.8\n",
      "stages.2.blocks.8.conv_dw\n",
      "stages.2.blocks.8.norm\n",
      "stages.2.blocks.8.mlp\n",
      "stages.2.blocks.8.mlp.fc1\n",
      "stages.2.blocks.8.mlp.act\n",
      "stages.2.blocks.8.mlp.drop1\n",
      "stages.2.blocks.8.mlp.norm\n",
      "stages.2.blocks.8.mlp.fc2\n",
      "stages.2.blocks.8.mlp.drop2\n",
      "stages.2.blocks.8.shortcut\n",
      "stages.2.blocks.8.drop_path\n",
      "stages.2.blocks.9\n",
      "stages.2.blocks.9.conv_dw\n",
      "stages.2.blocks.9.norm\n",
      "stages.2.blocks.9.mlp\n",
      "stages.2.blocks.9.mlp.fc1\n",
      "stages.2.blocks.9.mlp.act\n",
      "stages.2.blocks.9.mlp.drop1\n",
      "stages.2.blocks.9.mlp.norm\n",
      "stages.2.blocks.9.mlp.fc2\n",
      "stages.2.blocks.9.mlp.drop2\n",
      "stages.2.blocks.9.shortcut\n",
      "stages.2.blocks.9.drop_path\n",
      "stages.2.blocks.10\n",
      "stages.2.blocks.10.conv_dw\n",
      "stages.2.blocks.10.norm\n",
      "stages.2.blocks.10.mlp\n",
      "stages.2.blocks.10.mlp.fc1\n",
      "stages.2.blocks.10.mlp.act\n",
      "stages.2.blocks.10.mlp.drop1\n",
      "stages.2.blocks.10.mlp.norm\n",
      "stages.2.blocks.10.mlp.fc2\n",
      "stages.2.blocks.10.mlp.drop2\n",
      "stages.2.blocks.10.shortcut\n",
      "stages.2.blocks.10.drop_path\n",
      "stages.2.blocks.11\n",
      "stages.2.blocks.11.conv_dw\n",
      "stages.2.blocks.11.norm\n",
      "stages.2.blocks.11.mlp\n",
      "stages.2.blocks.11.mlp.fc1\n",
      "stages.2.blocks.11.mlp.act\n",
      "stages.2.blocks.11.mlp.drop1\n",
      "stages.2.blocks.11.mlp.norm\n",
      "stages.2.blocks.11.mlp.fc2\n",
      "stages.2.blocks.11.mlp.drop2\n",
      "stages.2.blocks.11.shortcut\n",
      "stages.2.blocks.11.drop_path\n",
      "stages.2.blocks.12\n",
      "stages.2.blocks.12.conv_dw\n",
      "stages.2.blocks.12.norm\n",
      "stages.2.blocks.12.mlp\n",
      "stages.2.blocks.12.mlp.fc1\n",
      "stages.2.blocks.12.mlp.act\n",
      "stages.2.blocks.12.mlp.drop1\n",
      "stages.2.blocks.12.mlp.norm\n",
      "stages.2.blocks.12.mlp.fc2\n",
      "stages.2.blocks.12.mlp.drop2\n",
      "stages.2.blocks.12.shortcut\n",
      "stages.2.blocks.12.drop_path\n",
      "stages.2.blocks.13\n",
      "stages.2.blocks.13.conv_dw\n",
      "stages.2.blocks.13.norm\n",
      "stages.2.blocks.13.mlp\n",
      "stages.2.blocks.13.mlp.fc1\n",
      "stages.2.blocks.13.mlp.act\n",
      "stages.2.blocks.13.mlp.drop1\n",
      "stages.2.blocks.13.mlp.norm\n",
      "stages.2.blocks.13.mlp.fc2\n",
      "stages.2.blocks.13.mlp.drop2\n",
      "stages.2.blocks.13.shortcut\n",
      "stages.2.blocks.13.drop_path\n",
      "stages.2.blocks.14\n",
      "stages.2.blocks.14.conv_dw\n",
      "stages.2.blocks.14.norm\n",
      "stages.2.blocks.14.mlp\n",
      "stages.2.blocks.14.mlp.fc1\n",
      "stages.2.blocks.14.mlp.act\n",
      "stages.2.blocks.14.mlp.drop1\n",
      "stages.2.blocks.14.mlp.norm\n",
      "stages.2.blocks.14.mlp.fc2\n",
      "stages.2.blocks.14.mlp.drop2\n",
      "stages.2.blocks.14.shortcut\n",
      "stages.2.blocks.14.drop_path\n",
      "stages.2.blocks.15\n",
      "stages.2.blocks.15.conv_dw\n",
      "stages.2.blocks.15.norm\n",
      "stages.2.blocks.15.mlp\n",
      "stages.2.blocks.15.mlp.fc1\n",
      "stages.2.blocks.15.mlp.act\n",
      "stages.2.blocks.15.mlp.drop1\n",
      "stages.2.blocks.15.mlp.norm\n",
      "stages.2.blocks.15.mlp.fc2\n",
      "stages.2.blocks.15.mlp.drop2\n",
      "stages.2.blocks.15.shortcut\n",
      "stages.2.blocks.15.drop_path\n",
      "stages.2.blocks.16\n",
      "stages.2.blocks.16.conv_dw\n",
      "stages.2.blocks.16.norm\n",
      "stages.2.blocks.16.mlp\n",
      "stages.2.blocks.16.mlp.fc1\n",
      "stages.2.blocks.16.mlp.act\n",
      "stages.2.blocks.16.mlp.drop1\n",
      "stages.2.blocks.16.mlp.norm\n",
      "stages.2.blocks.16.mlp.fc2\n",
      "stages.2.blocks.16.mlp.drop2\n",
      "stages.2.blocks.16.shortcut\n",
      "stages.2.blocks.16.drop_path\n",
      "stages.2.blocks.17\n",
      "stages.2.blocks.17.conv_dw\n",
      "stages.2.blocks.17.norm\n",
      "stages.2.blocks.17.mlp\n",
      "stages.2.blocks.17.mlp.fc1\n",
      "stages.2.blocks.17.mlp.act\n",
      "stages.2.blocks.17.mlp.drop1\n",
      "stages.2.blocks.17.mlp.norm\n",
      "stages.2.blocks.17.mlp.fc2\n",
      "stages.2.blocks.17.mlp.drop2\n",
      "stages.2.blocks.17.shortcut\n",
      "stages.2.blocks.17.drop_path\n",
      "stages.2.blocks.18\n",
      "stages.2.blocks.18.conv_dw\n",
      "stages.2.blocks.18.norm\n",
      "stages.2.blocks.18.mlp\n",
      "stages.2.blocks.18.mlp.fc1\n",
      "stages.2.blocks.18.mlp.act\n",
      "stages.2.blocks.18.mlp.drop1\n",
      "stages.2.blocks.18.mlp.norm\n",
      "stages.2.blocks.18.mlp.fc2\n",
      "stages.2.blocks.18.mlp.drop2\n",
      "stages.2.blocks.18.shortcut\n",
      "stages.2.blocks.18.drop_path\n",
      "stages.2.blocks.19\n",
      "stages.2.blocks.19.conv_dw\n",
      "stages.2.blocks.19.norm\n",
      "stages.2.blocks.19.mlp\n",
      "stages.2.blocks.19.mlp.fc1\n",
      "stages.2.blocks.19.mlp.act\n",
      "stages.2.blocks.19.mlp.drop1\n",
      "stages.2.blocks.19.mlp.norm\n",
      "stages.2.blocks.19.mlp.fc2\n",
      "stages.2.blocks.19.mlp.drop2\n",
      "stages.2.blocks.19.shortcut\n",
      "stages.2.blocks.19.drop_path\n",
      "stages.2.blocks.20\n",
      "stages.2.blocks.20.conv_dw\n",
      "stages.2.blocks.20.norm\n",
      "stages.2.blocks.20.mlp\n",
      "stages.2.blocks.20.mlp.fc1\n",
      "stages.2.blocks.20.mlp.act\n",
      "stages.2.blocks.20.mlp.drop1\n",
      "stages.2.blocks.20.mlp.norm\n",
      "stages.2.blocks.20.mlp.fc2\n",
      "stages.2.blocks.20.mlp.drop2\n",
      "stages.2.blocks.20.shortcut\n",
      "stages.2.blocks.20.drop_path\n",
      "stages.2.blocks.21\n",
      "stages.2.blocks.21.conv_dw\n",
      "stages.2.blocks.21.norm\n",
      "stages.2.blocks.21.mlp\n",
      "stages.2.blocks.21.mlp.fc1\n",
      "stages.2.blocks.21.mlp.act\n",
      "stages.2.blocks.21.mlp.drop1\n",
      "stages.2.blocks.21.mlp.norm\n",
      "stages.2.blocks.21.mlp.fc2\n",
      "stages.2.blocks.21.mlp.drop2\n",
      "stages.2.blocks.21.shortcut\n",
      "stages.2.blocks.21.drop_path\n",
      "stages.2.blocks.22\n",
      "stages.2.blocks.22.conv_dw\n",
      "stages.2.blocks.22.norm\n",
      "stages.2.blocks.22.mlp\n",
      "stages.2.blocks.22.mlp.fc1\n",
      "stages.2.blocks.22.mlp.act\n",
      "stages.2.blocks.22.mlp.drop1\n",
      "stages.2.blocks.22.mlp.norm\n",
      "stages.2.blocks.22.mlp.fc2\n",
      "stages.2.blocks.22.mlp.drop2\n",
      "stages.2.blocks.22.shortcut\n",
      "stages.2.blocks.22.drop_path\n",
      "stages.2.blocks.23\n",
      "stages.2.blocks.23.conv_dw\n",
      "stages.2.blocks.23.norm\n",
      "stages.2.blocks.23.mlp\n",
      "stages.2.blocks.23.mlp.fc1\n",
      "stages.2.blocks.23.mlp.act\n",
      "stages.2.blocks.23.mlp.drop1\n",
      "stages.2.blocks.23.mlp.norm\n",
      "stages.2.blocks.23.mlp.fc2\n",
      "stages.2.blocks.23.mlp.drop2\n",
      "stages.2.blocks.23.shortcut\n",
      "stages.2.blocks.23.drop_path\n",
      "stages.2.blocks.24\n",
      "stages.2.blocks.24.conv_dw\n",
      "stages.2.blocks.24.norm\n",
      "stages.2.blocks.24.mlp\n",
      "stages.2.blocks.24.mlp.fc1\n",
      "stages.2.blocks.24.mlp.act\n",
      "stages.2.blocks.24.mlp.drop1\n",
      "stages.2.blocks.24.mlp.norm\n",
      "stages.2.blocks.24.mlp.fc2\n",
      "stages.2.blocks.24.mlp.drop2\n",
      "stages.2.blocks.24.shortcut\n",
      "stages.2.blocks.24.drop_path\n",
      "stages.2.blocks.25\n",
      "stages.2.blocks.25.conv_dw\n",
      "stages.2.blocks.25.norm\n",
      "stages.2.blocks.25.mlp\n",
      "stages.2.blocks.25.mlp.fc1\n",
      "stages.2.blocks.25.mlp.act\n",
      "stages.2.blocks.25.mlp.drop1\n",
      "stages.2.blocks.25.mlp.norm\n",
      "stages.2.blocks.25.mlp.fc2\n",
      "stages.2.blocks.25.mlp.drop2\n",
      "stages.2.blocks.25.shortcut\n",
      "stages.2.blocks.25.drop_path\n",
      "stages.2.blocks.26\n",
      "stages.2.blocks.26.conv_dw\n",
      "stages.2.blocks.26.norm\n",
      "stages.2.blocks.26.mlp\n",
      "stages.2.blocks.26.mlp.fc1\n",
      "stages.2.blocks.26.mlp.act\n",
      "stages.2.blocks.26.mlp.drop1\n",
      "stages.2.blocks.26.mlp.norm\n",
      "stages.2.blocks.26.mlp.fc2\n",
      "stages.2.blocks.26.mlp.drop2\n",
      "stages.2.blocks.26.shortcut\n",
      "stages.2.blocks.26.drop_path\n",
      "stages.3\n",
      "stages.3.downsample\n",
      "stages.3.downsample.0\n",
      "stages.3.downsample.1\n",
      "stages.3.blocks\n",
      "stages.3.blocks.0\n",
      "stages.3.blocks.0.conv_dw\n",
      "stages.3.blocks.0.norm\n",
      "stages.3.blocks.0.mlp\n",
      "stages.3.blocks.0.mlp.fc1\n",
      "stages.3.blocks.0.mlp.act\n",
      "stages.3.blocks.0.mlp.drop1\n",
      "stages.3.blocks.0.mlp.norm\n",
      "stages.3.blocks.0.mlp.fc2\n",
      "stages.3.blocks.0.mlp.drop2\n",
      "stages.3.blocks.0.shortcut\n",
      "stages.3.blocks.0.drop_path\n",
      "stages.3.blocks.1\n",
      "stages.3.blocks.1.conv_dw\n",
      "stages.3.blocks.1.norm\n",
      "stages.3.blocks.1.mlp\n",
      "stages.3.blocks.1.mlp.fc1\n",
      "stages.3.blocks.1.mlp.act\n",
      "stages.3.blocks.1.mlp.drop1\n",
      "stages.3.blocks.1.mlp.norm\n",
      "stages.3.blocks.1.mlp.fc2\n",
      "stages.3.blocks.1.mlp.drop2\n",
      "stages.3.blocks.1.shortcut\n",
      "stages.3.blocks.1.drop_path\n",
      "stages.3.blocks.2\n",
      "stages.3.blocks.2.conv_dw\n",
      "stages.3.blocks.2.norm\n",
      "stages.3.blocks.2.mlp\n",
      "stages.3.blocks.2.mlp.fc1\n",
      "stages.3.blocks.2.mlp.act\n",
      "stages.3.blocks.2.mlp.drop1\n",
      "stages.3.blocks.2.mlp.norm\n",
      "stages.3.blocks.2.mlp.fc2\n",
      "stages.3.blocks.2.mlp.drop2\n",
      "stages.3.blocks.2.shortcut\n",
      "stages.3.blocks.2.drop_path\n",
      "norm_pre\n",
      "head\n",
      "head.global_pool\n",
      "head.global_pool.pool\n",
      "head.global_pool.flatten\n",
      "head.norm\n",
      "head.flatten\n",
      "head.pre_logits\n",
      "head.drop\n",
      "head.fc\n"
     ]
    }
   ],
   "source": [
    "# Layer name of each layer, used for deciding which layer to freeze\n",
    "for name, module in loaded_model_h5.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Decide which layer to freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head.fc.weight\n",
      "head.fc.bias\n"
     ]
    }
   ],
   "source": [
    "freeze_layers = [\"head.fc\"]\n",
    "# Output Layer name for which training is ON\n",
    "for name, parameter in loaded_model_h5.named_parameters():\n",
    "  if any(freeze_layer in name for freeze_layer in freeze_layers):\n",
    "    parameter.requires_grad = True\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layers = [loaded_model_h5.head.fc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "\n",
    "    # Process a PIL image for use in a PyTorch model\n",
    "\n",
    "    size = 256, 256\n",
    "    image.thumbnail(size, Image.LANCZOS)\n",
    "    image = image.crop((128 - 112, 128 - 112, 128 + 112, 128 + 112))\n",
    "    npImage = np.array(image)\n",
    "    npImage = npImage/255.\n",
    "\n",
    "    imgA = npImage[:,:,0]\n",
    "    imgB = npImage[:,:,1]\n",
    "    imgC = npImage[:,:,2]\n",
    "\n",
    "    imgA = (imgA - 0.485)/(0.229)\n",
    "    imgB = (imgB - 0.456)/(0.224)\n",
    "    imgC = (imgC - 0.406)/(0.225)\n",
    "\n",
    "    npImage[:,:,0] = imgA\n",
    "    npImage[:,:,1] = imgB\n",
    "    npImage[:,:,2] = imgC\n",
    "\n",
    "    npImage = np.transpose(npImage, (2,0,1))\n",
    "\n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2626848/2091123924.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  input_tensor = torch.FloatTensor([process_image(Image.open(image_path))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_tensor\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 10\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m grayscale_cam[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     13\u001b[0m visualization \u001b[38;5;241m=\u001b[39m show_cam_on_image(np\u001b[38;5;241m.\u001b[39masarray(Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)))\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255.\u001b[39m, grayscale_cam, use_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:192\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    190\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:105\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     94\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:137\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[1;32m    135\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 137\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                         \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    144\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:60\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_image\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m                   input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     54\u001b[0m                   target_layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m                   grads: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     58\u001b[0m                   eigen_smooth: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 60\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     weighted_activations \u001b[38;5;241m=\u001b[39m weights[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m activations\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eigen_smooth:\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/grad_cam.py:21\u001b[0m, in \u001b[0;36mGradCAM.get_cam_weights\u001b[0;34m(self, input_tensor, target_layer, target_category, activations, grads)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m                     input_tensor,\n\u001b[1;32m     17\u001b[0m                     target_layer,\n\u001b[1;32m     18\u001b[0m                     target_category,\n\u001b[1;32m     19\u001b[0m                     activations,\n\u001b[1;32m     20\u001b[0m                     grads):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/_methods.py:169\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    165\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    167\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    171\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[0;32m---> 77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[43mmu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     78\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "image_path = '/raid/ee-mariyam/maryam/ayush/TiH_Onion_data/Onion_Bulb_Rot/BR E5 P1 E (0070) - 24 Jan.JPG'\n",
    "input_tensor = torch.FloatTensor([process_image(Image.open(image_path))])\n",
    "image = Image.open(image_path)\n",
    "cam = GradCAM(model=loaded_model_h5, target_layers=target_layers)\n",
    "\n",
    "# targets = [ClassifierOutputTarget(10)]\n",
    "torch.set_grad_enabled(True)\n",
    "# grayscale_cam = cam(input_tensor=input_tensor, targets=targets)\n",
    "print(input_tensor.shape)\n",
    "grayscale_cam = cam(input_tensor=input_tensor)\n",
    "grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "visualization = show_cam_on_image(np.asarray(Image.open(image_path).resize((224, 224)))/255., grayscale_cam, use_rgb=True)\n",
    "plt.figure()\n",
    "plt.imshow(visualization)\n",
    "plt.figure()\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Generate CAM\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 35\u001b[0m     grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     grayscale_cam \u001b[38;5;241m=\u001b[39m grayscale_cam[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Convert the image to a numpy array and normalize it\u001b[39;00m\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:192\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    190\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:105\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     94\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_cam_per_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:137\u001b[0m, in \u001b[0;36mBaseCAM.compute_cam_per_layer\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads_list):\n\u001b[1;32m    135\u001b[0m     layer_grads \u001b[38;5;241m=\u001b[39m grads_list[i]\n\u001b[0;32m--> 137\u001b[0m cam \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_activations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mlayer_grads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m                         \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m cam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(cam, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    144\u001b[0m scaled \u001b[38;5;241m=\u001b[39m scale_cam_image(cam, target_size)\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/base_cam.py:60\u001b[0m, in \u001b[0;36mBaseCAM.get_cam_image\u001b[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_image\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m                   input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     54\u001b[0m                   target_layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m                   grads: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     58\u001b[0m                   eigen_smooth: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 60\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cam_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtarget_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     weighted_activations \u001b[38;5;241m=\u001b[39m weights[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m activations\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m eigen_smooth:\n",
      "File \u001b[0;32m~/ayush/./pytorch-grad-cam/pytorch_grad_cam/grad_cam.py:21\u001b[0m, in \u001b[0;36mGradCAM.get_cam_weights\u001b[0;34m(self, input_tensor, target_layer, target_category, activations, grads)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cam_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m                     input_tensor,\n\u001b[1;32m     17\u001b[0m                     target_layer,\n\u001b[1;32m     18\u001b[0m                     target_category,\n\u001b[1;32m     19\u001b[0m                     activations,\n\u001b[1;32m     20\u001b[0m                     grads):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3464\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3464\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/_methods.py:169\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    165\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    167\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    171\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ayush/lib/python3.8/site-packages/numpy/core/_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[0;32m---> 77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[43mmu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     78\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Define the image processing function\n",
    "def process_image(img):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Load your model (make sure it's on the same device as your input tensor)\n",
    "# loaded_model_h5 = torch.load('your_model.pth')\n",
    "# loaded_model_h5.eval()\n",
    "\n",
    "# Define the target layers\n",
    "target_layers = [loaded_model_h5.head.fc]  # Modify this to match your model's architecture\n",
    "\n",
    "# Load and preprocess the image\n",
    "image_path = '/raid/ee-mariyam/maryam/ayush/TiH_Onion_data/Onion_Bulb_Rot/BR E5 P1 E (0070) - 24 Jan.JPG'\n",
    "image = Image.open(image_path)\n",
    "input_tensor = process_image(image)\n",
    "\n",
    "# Initialize GradCAM\n",
    "cam = GradCAM(model=loaded_model_h5, target_layers=target_layers)\n",
    "\n",
    "# Generate CAM\n",
    "with torch.set_grad_enabled(True):\n",
    "    grayscale_cam = cam(input_tensor=input_tensor)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "# Convert the image to a numpy array and normalize it\n",
    "image = np.asarray(image.resize((224, 224))) / 255.0\n",
    "\n",
    "# Ensure the image has three channels\n",
    "if len(image.shape) == 2:\n",
    "    image = np.repeat(image[:, :, np.newaxis], 3, axis=2)\n",
    "\n",
    "# Create the visualization\n",
    "visualization = show_cam_on_image(image, grayscale_cam, use_rgb=True)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure()\n",
    "plt.imshow(visualization)\n",
    "plt.figure()\n",
    "plt.imshow(Image.open(image_path))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMap:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate(self, input_tensor):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "        \n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        attention_map = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            attention_map += w * activations[i]\n",
    "\n",
    "        attention_map = np.maximum(attention_map, 0)\n",
    "        attention_map = attention_map / np.max(attention_map)  # Normalize to [0, 1]\n",
    "        return attention_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = '/raid/ee-mariyam/maryam/ayush/TiH_Onion_data/Onion_Bulb_Rot/BR E5 P1 E (0070) - 24 Jan.JPG'\n",
    "img = Image.open(img_path)\n",
    "input_tensor = preprocess(img).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Generate attention map\u001b[39;00m\n\u001b[1;32m     86\u001b[0m attention_map_generator \u001b[38;5;241m=\u001b[39m AttentionMap(model, target_layer)\n\u001b[0;32m---> 87\u001b[0m attention_map \u001b[38;5;241m=\u001b[39m \u001b[43mattention_map_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Remove hooks after generating attention map\u001b[39;00m\n\u001b[1;32m     90\u001b[0m attention_map_generator\u001b[38;5;241m.\u001b[39mremove_hooks()\n",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m, in \u001b[0;36mAttentionMap.generate\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     48\u001b[0m activations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(gradients, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     52\u001b[0m attention_map \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(activations\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the ConvNeXt model\n",
    "from timm import create_model\n",
    "\n",
    "# # Load the ConvNeXt model\n",
    "# model = create_model('convnext_base', pretrained=True)\n",
    "# model.eval()\n",
    "\n",
    "# Define a class to compute the attention map\n",
    "class AttentionMap:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output\n",
    "\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0]\n",
    "\n",
    "        self.hooks = []\n",
    "        for name, module in self.model.named_modules():\n",
    "            if module == self.target_layer:\n",
    "                self.hooks.append(module.register_forward_hook(forward_hook))\n",
    "                self.hooks.append(module.register_backward_hook(backward_hook))\n",
    "                break\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def generate(self, input_tensor):\n",
    "        self.model.zero_grad()\n",
    "        output = self.model(input_tensor)\n",
    "        loss = output.mean()\n",
    "        loss.backward()\n",
    "\n",
    "        activations = self.activations.detach().cpu().numpy()[0]\n",
    "        gradients = self.gradients.detach().cpu().numpy()[0]\n",
    "\n",
    "        weights = np.mean(gradients, axis=(1, 2))\n",
    "        attention_map = np.zeros(activations.shape[1:], dtype=np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            attention_map += w * activations[i]\n",
    "\n",
    "        attention_map = np.maximum(attention_map, 0)\n",
    "        attention_map = attention_map / np.max(attention_map)  # Normalize to [0, 1]\n",
    "        return attention_map\n",
    "\n",
    "# Define image transformations\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = '/raid/ee-mariyam/maryam/ayush/TiH_Onion_data/Onion_Bulb_Rot/BR E5 P1 E (0070) - 24 Jan.JPG'\n",
    "img = Image.open(img_path)\n",
    "input_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "# Identify the target layer for attention map computation\n",
    "target_layer = None\n",
    "\n",
    "# Traverse through the model to find the target layer (assuming it's one of the last convolutional layers)\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        target_layer = module\n",
    "        break\n",
    "\n",
    "if target_layer is None:\n",
    "    raise RuntimeError(\"Target layer not found. Ensure the model structure is compatible.\")\n",
    "\n",
    "# Generate attention map\n",
    "attention_map_generator = AttentionMap(model, target_layer)\n",
    "attention_map = attention_map_generator.generate(input_tensor)\n",
    "\n",
    "# Remove hooks after generating attention map\n",
    "attention_map_generator.remove_hooks()\n",
    "\n",
    "# Visualize the attention map\n",
    "plt.imshow(attention_map, cmap='hot', interpolation='nearest')\n",
    "plt.axis('off')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ayush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
